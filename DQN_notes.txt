OveralFor building RL environment
	- (Action) BoundedArraySpec, it's np.brodcast
		- take each minimum and maximum and makes it to a matrix of the given shape (if provided)
		- OR take the range of minimum and maximum and spread it 
		- I guess you can represent any 3D environment like that 
	- (Observation) ArraySpec
		- are the features length
	id = number of instances
	? episode_step = 0  # Episode step, resets every episode
	state = is the instance
	step
		y label is acted as reward to the action (BoundedArraySpec [1,0])
		then the reward is given based on the observation which are the features
		
		
RL_1 environment
	- Observation
		- Is the NE with some features (embedding) like in the IMDB
	- Action
		- Is [0,1]
	- Reward
	- Replay Buffer
		- https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c
			- We can prevent action values from oscillating or diverging catastrophically using a large 
				buffer of our past experience and sample training data from it, instead of using our latest experience.
			- The replay buffer contains a collection of experience tuples (S, A, R, S′)
			- The act of sampling a small batch of tuples from the replay buffer in order to learn is known as experience replay
		- We do this for every recipe.
	- id
		- is a random NE word
	- We give reward after each episode - a recipe
	- We give reward after each epoch - all the recipes are processed (LM type)
	- Balancing Multiple Sources of Reward in Reinforcement Learning (2000)
		- follow up
	- DCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING (2017)
		- seems similar that it has many QAs
	- Multi-agent: with sharing "logits"
	- Do I need batched size
	- I want rather TF environment rather than Python 
		- https://www.tensorflow.org/agents/tutorials/2_environments_tutorial#tensorflow_environments
	- I think this looks similar to what I want
		- Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery (2019)
			-https://github.com/011235813/hierarchical-marl
				- TF with no RL library
	- MARL papers
		- https://github.com/LantaoYu/MARL-Papers
	- Algorithm
		- Take a random seed which has not been seed before and prioritise non tagged NEs
		- For each random seed:
			- Take as action observation its neighbour based on normal distribution
				- The neighbour should not be a previous seed but it can be a previous tagged NE just not an observation that has been already on the same episode
		- Actions
			- To include or not (0,1)
		- Rewards
			- Penalise when adding a non NE
			- Penalise and reward episodes that are high or not all convoluted
			- Penalise by taking into account the number of main verbs of a recipe and that each episode should have the numer of NEs/main verbs
            - Use vanilla vs pre-train BERT to estimate the correct rewards (based on the paper: Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning )
                - For starters set all rewards equally divided by the total and 0 for the Os.
		- Rewards after many episodes, or epochs
			- For a certain group of recipes
				- Concatenate all the events, ordered-lessly
				- Build an MDP, LM for these events
				- If in the eval the MDP (LM) perfoms good then reward
				- We can also penalise if the MDP(LM) performs better in other domains instead of it's own
		- embeddings
			- contextualies with the sentence they belong to
				- or with Dep Tag
			- with sentence and recipe position
			- with the word itself - pretrain
		
Websites I am viewing:
https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb
https://github.com/tensorflow/agents/tree/master/docs/tutorials
https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb
	
General
https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e

https://ai.stackexchange.com/questions/11350/reinforcement-learning-with-long-term-rewards-and-fixed-states-and-actions
    Only one non-zero reward per thousand steps - this is referred to as "sparse rewards"
    I can return zero reward if chosen a simple NE
    Is called also: temporal credit assignment problem
        Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning (2019)
             (We presented a new algorithm for learning temporal credit assignment, which uses deep neural network (Transformer) to decompose the episodic reward back to each time step in the trajectory)
            - https://github.com/aadeshnpn/Temporal-Credit-Assignment
                - Downloaded on Zip

Discount factor gamma
    - https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning
        - I think I need a higher (closer to one) since I emphasize on future rewards

Experience Replay
    https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits
        The learning phase is then logically separate from gaining experience, and based on taking random samples from this table.
        It is harder to use multi-step learning algorithms
            - Search that
    https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c
        - Replay buffer or Experience buffer
            - We can prevent action values from oscillating or diverging catastrophically using a large buffer of our past experience and sample training data from it, instead of using our latest experience.
        - The act of sampling a small batch of tuples from the replay buffer in order to learn is known as experience replay.
        - the basic idea behind experience replay is to storing past experiences and
            - then using a random subset of these experiences to update the Q-network,
                - rather than using just the single most recent experience
        - Is done using deque

        + Target Network
            - To make training more stable, there is a trick, called target network, by which we keep a copy of our neural network and use it for the Q(s’, a’) value in the Bellman equation.
                -  the target network’s parameters are not trained, but they are periodically synchronized with the parameters of the main Q-network

Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning (2019)
    - straightforward application of policy gradient methods, including REINFORCE [41], A2C [24] and PPO [32], may suffer from sample inefficiency, as the final episodic reward would only provide the same or similar supervision for learning policy over all time steps in a trajectory.
    - blackbox optimization approaches , such as cross entropy method [29], CMA-ES [11] and evolution strategies [30], have been also applied to episodic RL problems due to their computational efficiency.
    - Have a predictor that finds the best distribution of rewards in the action steps by minimising to the episodic reward (or the IntervalS)
    - What I want is an intermittent TCA since some are non NEs

    Learning Guidance Rewards with Trajectory-space Smoothing (2020)
        - Cite the above
        - In contrast with these, our computation of the guidance rewards does not require training auxiliary networks and could be viewed as a simple uniform return decomposition
        - A favorable property of our approach is that no additional neural networks need to be trained to obtain the guidance rewards, unlike recent works that also examine RL with delayed rewards
            - Will have https://github.com/tgangwani/GuidanceRewards

    Agent57: Outperforming the Atari Human Benchmark (2020)
        - Cite the above
        - Game of skiing seems relevant since it gets -5 after missing gates then receives in the end a final reward
        - Exploration algorithms in deep RL
            - NGU (Never Give Up)
        - Contributions
            - A new parameterization of the state-action value function that decomposes the contributions of the intrinsic and extrinsic rewards. As a result, we significantly increase the training stability over a large range of intrinsic reward scales.
            - This allows the agent to control the exploration/exploitation trade-off by dedicating more resources to one or the other.

