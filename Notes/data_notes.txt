Not detected cooking devices:
    skillet, colander, fork, paper towel,

Verbs:
    - conditioned should not be verbs (e.g., browned)


Checked some devices and excluded some wrongly setted as devices

Skipping the IOB/2 format for now.
    - Does using IOB improves accuracy?

In getting the HuggingFace embeddings we have to match to spaCy's
    Because of tokenisation, spaCy groups larger set of characters and HuggingFace considers only the first set
    Then to not cause an error we match with the character index to the HuggingFace's token

not device:
    balls
    bit
    base
    rest

device:
    parchment
    sheets
    fork
    forks
    bottle
    container
    ramekins
    boiler
    toothpick

In the below the word "Filling" in its second instance was mistaken in the annotation and assigned with the index of its first instance.
    Preheat oven to 350 degrees F (175 degrees C). Lightly grease baking sheets. Cream butter or margarine with the brown sugar until light and fluffy. Beat in the eggs and mix well. Stir in the salt, cinnamon, baking powder, boiling water, baking soda and flour. Mix to combine. Stir in the oats. Drop cookies onto the prepared baking sheets. Bake at 350 degrees F (175 degrees C) for 10 to 12 minutes. Let cookies cool then make sandwiches from two cookies filled with Whoopie Pie Filling. To Make Filling: Beat egg white. Mix in the milk, 1 teaspoon vanilla and 1 cup confectioner s sugar. Beat in the shortening and remaining 1 cup confectioner s sugar. Beat until light.

Produced memory error when compiling dataset:
    - tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[305,378,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Pack] name: stack
        OP_REQUIRES failed at pack_op.cc:75 : Resource exhausted: OOM when allocating tensor with shape[305,378,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
        Traceback (most recent call last):
        2021-07-06 01:29:43.491809: W tensorflow/core/common_runtime/bfc_allocator.cc:456] Allocator (GPU_0_bfc) ran out of memory trying to allocate 337.76MiB (rounded to 354170880)requested by op Pack
        If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.
