- Set reward 0 if non-NE and divide by the total in the end of the Interval for the rest.
- Search code on other libs on how the deal with the TCA
- Read chpater 5 https://ebookcentral.proquest.com/lib/manchester/reader.action?docID=5439844&ppg=104
- How to establish TCA, after how many runs should be random?
- RL when actions depend on states
- Read about RL in NLP how the action verbs are taken,
    - There is no state exploration
- The different RL algorithms: actor-ctitic, reinforce, ...
Autoencoders
RL LM
https://arxiv.org/pdf/1906.03926.pdf
On vs Off policy?
Continuous states
NER + RL -> more
survey on RL and approximator functions which is best to use, how to train, which are SOTA to represent the state
check Gmail (done)
- https://github.com/rmsander/marl_ppo/blob/main/ppo/utils.py
    - Can run and see if it works
-----------------------------------------------------------------------------
Update Env
    - Change to consistent exploration path
Update Data
Update Driver

How to do Autoencoders to events, have event embeddings with sequence knowledge and also be orderless in the matters of lexical units
    If we have the embeddings then we can cluster them
    See the Neurips Tutorial on similar ingredients
    How to concatenate, can have different approaches and then track all of them
        I can have different dimensions based on the NEs type (i.e, INGR, Tool) to form a representation of an event in these axis


Have to have an agent thread which starts with a non NE and then comes suddenly to an end

Is it offline, predetermined states but not actions
    What is the best agent for that
        - The closest example to mine: https://ai.stackexchange.com/questions/11350/reinforcement-learning-with-long-term-rewards-and-fixed-states-and-actions
        - Which seems okay in my case of having fixed states, it says that makes the Bellman equation simpler, but still I am not sure if I can advantage of offline RL
    - Ask later on SE:
        - Is this a half Offline Reinforcement Learning problem? (states are fixed but not actions)
           - In a typical RL environment, the policy chooses an action and depending on that there is a reward and the transition to the next state provided by the environment.
           - What if the course (trajectory) of states are predetermined? Each state goes after another no matter the action, but still, the action plays a key role in obtained reward. The objective is to find the best set of actions given a  current set of states (the states change in each episode).
           - From what I get in offline RL, which can be used in recorded data, the states are fixed and the actions. In my scenario, I describe only that the states are fixed. So what exactly is this problem and how do I approach it? Can I use offline RL techniques, or am I knocking on the wrong door?

Partial observable MDP?
    Meaning to know always the first label but then the intermediate to ignore sometimes in order to learn better from context a specific relation but not with the sequence. Search this concept.

Start paper

- Find which VAE is best suited, that I can find similar ingredients, maybe also support hierarchies, has code
- What would be the fallback option?
    - Concatenate each event based on lemma and hypernym, make a dictionary of those and then learn the sequences, via RNN?


Code
    - What is the time_step_spec exactly? Set it to Tensor to match with the rest

Check termination vs truncation
how to start and stop in one step

Check if can read from replay_buffer
Check the actions, how is -1 treated

Fix agent or change
Check trajs