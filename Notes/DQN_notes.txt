OveralFor building RL environment
	- (Action) BoundedArraySpec, it's np.brodcast
		- take each minimum and maximum and makes it to a matrix of the given shape (if provided)
		- OR take the range of minimum and maximum and spread it 
		- I guess you can represent any 3D environment like that 
	- (Observation) ArraySpec
		- are the features length
	id = number of instances
	? episode_step = 0  # Episode step, resets every episode
	state = is the instance
	step
		y label is acted as reward to the action (BoundedArraySpec [1,0])
		then the reward is given based on the observation which are the features
		
		
RL_1 environment
	- Observation
		- Is the NE with some features (embedding) like in the IMDB
	- Action
		- Is [0,1]
	- Reward
	- Replay Buffer
		- https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c
			- We can prevent action values from oscillating or diverging catastrophically using a large 
				buffer of our past experience and sample training data from it, instead of using our latest experience.
			- The replay buffer contains a collection of experience tuples (S, A, R, S′)
			- The act of sampling a small batch of tuples from the replay buffer in order to learn is known as experience replay
		- We do this for every recipe.
	- id
		- is a random NE word
	- We give reward after each episode - a recipe
	- We give reward after each epoch - all the recipes are processed (LM type)
	- Balancing Multiple Sources of Reward in Reinforcement Learning (2000)
		- follow up
	- DCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING (2017)
		- seems similar that it has many QAs
	- Multi-agent: with sharing "logits"
	- Do I need batched size
	- I want rather TF environment rather than Python 
		- https://www.tensorflow.org/agents/tutorials/2_environments_tutorial#tensorflow_environments
	- I think this looks similar to what I want
		- Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery (2019)
			-https://github.com/011235813/hierarchical-marl
				- TF with no RL library
	- MARL papers
		- https://github.com/LantaoYu/MARL-Papers
	- Algorithm
		- Take a random seed which has not been seed before and prioritise non tagged NEs
		- For each random seed:
			- Take as action observation its neighbour based on normal distribution
				- The neighbour should not be a previous seed but it can be a previous tagged NE just not an observation that has been already on the same episode
		- Actions
			- To include or not (0,1)
		- Rewards
			- Penalise when adding a non NE
			- Penalise and reward episodes that are high or not all convoluted
			- Penalise by taking into account the number of main verbs of a recipe and that each episode should have the numer of NEs/main verbs
            - Use vanilla vs pre-train BERT to estimate the correct rewards (based on the paper: Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning )
                - For starters set all rewards equally divided by the total and 0 for the Os.
		- Rewards after many episodes, or epochs
			- For a certain group of recipes
				- Concatenate all the events, ordered-lessly
				- Build an MDP, LM for these events
				- If in the eval the MDP (LM) perfoms good then reward
				- We can also penalise if the MDP(LM) performs better in other domains instead of it's own
		- embeddings
			- contextualies with the sentence they belong to
				- or with Dep Tag
			- with sentence and recipe position
			- with the word itself - pretrain
		
Websites I am viewing:
https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb
https://github.com/tensorflow/agents/tree/master/docs/tutorials
https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb
	
General
https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e

https://ai.stackexchange.com/questions/11350/reinforcement-learning-with-long-term-rewards-and-fixed-states-and-actions
    Only one non-zero reward per thousand steps - this is referred to as "sparse rewards"
    I can return zero reward if chosen a simple NE
    Is called also: temporal credit assignment problem
        Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning (2019)
             (We presented a new algorithm for learning temporal credit assignment, which uses deep neural network (Transformer) to decompose the episodic reward back to each time step in the trajectory)
            - https://github.com/aadeshnpn/Temporal-Credit-Assignment
                - Downloaded on Zip

Discount factor gamma
    - https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning
        - I think I need a higher (closer to one) since I emphasize on future rewards
        - Or a zero since, the states are not inter-depended

Experience Replay
    https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits
        The learning phase is then logically separate from gaining experience, and based on taking random samples from this table.
        It is harder to use multi-step learning algorithms
            - Search that
    https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c
        - Replay buffer or Experience buffer
            - We can prevent action values from oscillating or diverging catastrophically using a large buffer of our past experience and sample training data from it, instead of using our latest experience.
        - The act of sampling a small batch of tuples from the replay buffer in order to learn is known as experience replay.
        - the basic idea behind experience replay is to storing past experiences and
            - then using a random subset of these experiences to update the Q-network,
                - rather than using just the single most recent experience
        - Is done using deque

        + Target Network
            - To make training more stable, there is a trick, called target network, by which we keep a copy of our neural network and use it for the Q(s’, a’) value in the Bellman equation.
                -  the target network’s parameters are not trained, but they are periodically synchronized with the parameters of the main Q-network
    https://github.com/tensorflow/agents/blob/master/docs/tutorials/5_replay_buffers_tutorial.ipynb
        - I think with the Replay Buffer I can run it after defined n steps for the RL to learn.
            - Which fits my case.

Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning (2019)
    - straightforward application of policy gradient methods, including REINFORCE [41], A2C [24] and PPO [32], may suffer from sample inefficiency, as the final episodic reward would only provide the same or similar supervision for learning policy over all time steps in a trajectory.
    - blackbox optimization approaches , such as cross entropy method [29], CMA-ES [11] and evolution strategies [30], have been also applied to episodic RL problems due to their computational efficiency.
    - Have a predictor that finds the best distribution of rewards in the action steps by minimising to the episodic reward (or the IntervalS)
    - What I want is an intermittent TCA since some are non NEs

    Learning Guidance Rewards with Trajectory-space Smoothing (2020)
        - Cite the above
        - In contrast with these, our computation of the guidance rewards does not require training auxiliary networks and could be viewed as a simple uniform return decomposition
        - A favorable property of our approach is that no additional neural networks need to be trained to obtain the guidance rewards, unlike recent works that also examine RL with delayed rewards
            - Will have https://github.com/tgangwani/GuidanceRewards

    Agent57: Outperforming the Atari Human Benchmark (2020)
        - Cite the above
        - Game of skiing seems relevant since it gets -5 after missing gates then receives in the end a final reward
        - Exploration algorithms in deep RL
            - NGU (Never Give Up)
        - Contributions
            - A new parameterization of the state-action value function that decomposes the contributions of the intrinsic and extrinsic rewards. As a result, we significantly increase the training stability over a large range of intrinsic reward scales.
            - This allows the agent to control the exploration/exploitation trade-off by dedicating more resources to one or the other.

Policies
    - https://stackoverflow.com/a/46269757/5455013
        - Which action should the RL take?
        - I think in my case, start with random

https://github.com/tensorflow/agents/blob/master/docs/tutorials/4_drivers_tutorial.ipynb
    - Drivers execute a policy over a specific number of steps or episodes. Should I use that in my task?
        - Does the RL learn after the driver has been executed? Is the reward given after that?

States could depend on the action taken, in my case it does not depend

Offline RL (https://slideslive.com/38935785/offline-reinforcement-learning-from-algorithms-to-practical-challenges)
    - good for when to prioritise distributions over good and bad behaviours
        - For tweaking the policy
    - Good for when there is alot of data on a subject like self driving cars
        - Make use of offline data, instead of online
        - There could be a lot of data show the correct trajectory but the agent cannot test other trajectories
            - leading to compounding errors
        - Distribution shift is an issue, see minute 40 for a good example
    - I think offline RL is not useful here, follow up later
    - paper resource: https://arxiv.org/pdf/2005.01643.pdf (Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems)
    - https://github.com/tensorflow/agents/issues/405
    - I think offline RL is similar because the agent cannot choose the new state. But is different because it can choose the action which changes the reward.

- Read chpater 5 https://ebookcentral.proquest.com/lib/manchester/reader.action?docID=5439844&ppg=104
    - TD Learning
        - approximates current estimate based on previous learned estimates, unlike Monte Carlo
            - (a.k.a. boostraping)
        - MC takes the mean return
        - It is about updating the reward based on the next state
            - Looks for actions that has the highest reward then it takes that action leading to a new state
        - I think it does not apply to our case, since whichever action we take it will always land to the same new state.
        - Sarsa vs Q learning I think differs on exploring options rather than taking the max
            - Exactly, the difference between on and off policy. Sarsa takes the future state reward whereas Q learning sets what was the max there.

Chapter 6 https://ebookcentral.proquest.com/lib/manchester/reader.action?docID=5439844&ppg=104
    - MAB
        - Different exploration techniques
Chapter 8
    - I think is about the expansions of chapter 5 Q network with more Deep version where the states are convoluted.
Chapter 9
    - partially observable Markov Decision Process ( POMDP )
        - is when we only know part of the world
    - Uses LSTM to model a state with the previous states
    - With Attention as well
    - I could use something similar between each state of words?

Chapter 10
    - A3C, many agents which update the global network
        - Cause of that multi-agent structure no use of replay memory

Chapter 12 Racing
    - I think I can use the target network to update the rewards in my case?




Extracting Action Sequences from Texts Based on Deep Reinforcement Learning (2018)
    - Q-networks to build the Q-functions.
    - trains two networks one for verbs and one for their args, where the latter uses the input of the first
    - CNN
        - design four types of kernels, which correspond to bigram, trigram, four-gram and five-gram, respectively.]
    https://github.com/Fence/EASDRL/blob/master/Environment.py
        - elif set rewards
        - Move to new state
            - self.state[word_ind, -1] = action + 1
        - self.state = self.text_vec.copy()
            - I assume the step just goes to the next word

I think my case looks like the MAB
    - https://stats.stackexchange.com/questions/379644/actions-with-no-effect-on-the-enviroment-in-rl

RL_algos procedure:
    - Policy
        - which binary action to take?
    - Exploration
        - Sequential, Normal distribution or other?
    -

Sequence labeling with Reinforcement Learning and Ranking Algorithms (2007)
    - Order the ones first which our agent is the most certain

Continuous space
    - https://ai.stackexchange.com/questions/12255/can-q-learning-be-used-for-continuous-state-or-action-spaces
    - Do I need to encode the embeddings to a sub states or use an Deep learning mapping function

Ensemble NER with Q Network
    - https://link-springer-com.manchester.idm.oclc.org/chapter/10.1007/978-3-030-29894-4_13
    - Document-Level Named Entity Recognition with Q-Network
    - His code is weird

Joint Entity and Relation Extraction with a Hybrid Transformer and Reinforcement Learning Based Model (2020)
    - Considers one triplet possible per sentence
    - I think it uses a approximator function for the state, which takes into account different sentence features
    - I think does not have code

DeNERT-KG: Named Entity and Relation Extraction Model Using DQN, Knowledge Graph, and BERT (2020)
    - An epoch is when all sequences are labelled in a sentece


Deep Reinforcement Learning with a Natural Language Action Space (2016)
    - DRRN function approximator
        - to train
            - Uses experience replay
                -  compute the temporal difference error for sample k
            - Can my sentence embeddings algo help to converge the space
        - It's about text based RPG games

Distantly Supervised NER with Partial Annotation Learning and Reinforcement Learning (2018)
    - Selecting clean instances for the NER, not very clear, maybe revisit later
        - https://github.com/rainarch/DSNER

NATURAL LANGUAGE STATE REPRESENTATION FOR REINFORCEMENT LEARNING (2019 ICLR)
    - good to cite, it seems that language is better for RL cause is more concise
        - was rejected

Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games (2020)
    - it's a game setting
        - maybe revisit later

Delayed rewards
    - https://github.com/tensorflow/agents/issues/529

In Conversational AI
    - The state is constructed from the information the last User action as well as the last Agent action.

RL tutorial
    https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#FunctionApproximation


Curriculum Learning
    - Is an interesting section when learning the cooking actions
    - Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey
        - Is about having a teacher and learning the best sequence of actions


https://medium.com/swlh/states-observation-and-action-spaces-in-reinforcement-learning-569a30a8d2a1
    -  For example the DDPG algorithm can only be applied to environments with continuous action space, while the PPO algorithm can be used for environments with either discrete or continuous action spaces.
    - I try an agent for continuous envs

https://github.com/ConvLab/ConvLab/

Some DRL for Dialogues just add the word embeddings together for the states, plus concatenate the previous state
    - https://github.com/KGSands/DRL-Chatbot/blob/master/chatbot.py

PPO agent contructor
    - https://github.com/rmsander/marl_ppo/blob/main/ppo/utils.py

Objective
    - Cluster (same events) words (tokens) from sentences (sequnces)

A non-RL approach
    - What if we learn to classify relatiions between the NEs, similarly to the SOTA NER, RE
        - We wouldn't be able to exploit the rewards from our low level penalisations and the event sequence reward
            - The RL gives us the tool to explore different options guided by the penalites-rewards

Discount Gamma should be big i think. Because we want to have a throughout good path of 0s and 1s

Path, Exploration route
     Has to be consistent pattern like right left, right + 1, left + 1. So the agent learns the positions. Otherwise, different positions with change of words may cause confusion.

Is it offline RL?
    If we had a disctionary of words, their embeddings and an NER algo, we could have a proper RL env, where the agent selects which word could go next and then with the NER calculate the reward. Our example is the RL in a car racing env, but we have defined path, the only thing is to find the actions. If only states are bounded is it still offline RL?

The approach is to do the lower penalisation. The train the agent. Apply greedy policy, then evaluate with event sequence. Afterwards, keep checking mostly close to greedy policies in order to not deviate from the lower penalisation rules.

How to set in RL if first state is 0 then terminate episode?

What if we have partial observable MDP. Meaning to know always the first label but then the intermediate to ignore sometimes in order to learn better from context a specific relation but not with the sequence. Search this concept.

I can try the event sequencing with different set of embeddings and concatenations. Then track score for each combination and see how they work.

Relevant applications or fine-tuning word embeddings
    - Construct events
        - Have a higher penalisation
            - ABSA reviews
                - avail: different products (domain), review stars
                    - Learn to predict aspect, sentiment pair and their review
                    - Learn NER from domain shifts
    - Create Ontologies maybe

Choosing an algorithm
    - https://intellabs.github.io/coach/selecting_an_algorithm.html

Very similar example
    https://openreview.net/pdf?id=BJlsVZ966m

Embeddings inspiration (to read)
    - https://www.semanticscholar.org/paper/Joint-Event-Extraction-Model-based-on-Multi-feature-Shuo-Yuan/a70865682a51b09635bc6f927ddd7abf66e8a9c0
        - Joint Event Extraction Model based on Multi-feature Fusion
    - https://www.semanticscholar.org/paper/Improving-Event-Detection-using-Contextual-Word-and-Maisonnave-Delbianco/ce6816071fbc8329a7daffabf3efc60284c45533
        - Improving Event Detection using Contextual Word and Sentence Embeddings

Other papers that seem bit relevant
    - Enhanced prototypical network for few-shot relation extraction
    - Entity, Relation, and Event Extraction with Contextualized Span Representations
        - https://github.com/dwadden/dygiepp
    - ENPAR:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction
        - https://github.com/Receiling/ENPAR (is missing)
        - Is interesting paper, using Ber's MLM, NSP methodology to code entities and their predictions instead
    - https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16257/16125
        - RL for RE with exogenous data KB
            - Seet the citations for more similar RL for RE

Inverse RL
    - when we have the paths from an expert (bees), but we do not have the rewards

Event Extraction with Generative Adversarial Imitation Learning
    - uses IRL to find the rewards
    - It does this by using GAIL (Generative Adversarial Imitation Learning)

How to deal with variable action set
    - In my example some states may be zero and no action is to be taken
        - https://ai.stackexchange.com/questions/9491/any-papers-regarding-different-inconsistent-action-space-in-reinforcement-learni
    - Go with that approach for now
        - Assuming the number of actions is not too big, you can simply ignore actions which don't apply in a given state. That's different from learning - you don't have to learn returning negative award for illegal actions, you simply don't care and select the legal action returning the best award.


What if the action is 0, Verb, Ingre, Tool instead of 0,1 in this way the RL learns all the various roles and will use them for event representation

event representation
    - have a distance between each NE (verb, ingr) of the predicted and target event
        - Count the distances
    -  Can use simple Glove Embeddings on the hypernyms
        - take lemma of each NE
    - How to combine mulitple NEs of the same type
        - Use some custom heuristics for starters
            - the common hypernym, or average of tw
    - What it we use some sort of clustering techinque to cut down the amount of unique words
        - Probably a distane based clustering techinque
    - Need to have a function to calculate the event average position
    - Use VAE to generate also for each type, nested type
        - Or could be to bring the threshold to a level where if it's above that then include those
        - Could apply also some common sense rules when generating NEs
    - Encoding and then feeding to whatever sequence learner
        - Can easily encode different types
        - Have the risk of runing into too many different types making it not enough data to produce
            - A workaround could be to cluster the NEs
                - Cluster each NE type
                    - To cluster ingrs
                        - First find the average of each embedding ingr group (based on labels)
                        - Then cluster
            - For multiple NEs
                - Do one pass after the training and identify the common events which have at least 2, 3 (apriori algorithm)
                - Encode the events with the most possible matches
            - N-grams of NEs
                - take the average I guess

Event Methodology
    - From threads encode an event
        -
    - Build a prediction model
    - Evaluate on validation set
        - Build threads
        - Encode them to events
        - Make a Cloze or prediction evaluation

Event sequence representation
    - CRF, RNN
    - Use GloVe embeddings
    - encode each unique event to an embedding
        - Could have the GloVe embeddings as features? (in CRF)

Use HuggingFace embeddings
Spacy I think does not have it's own pre-trained embeddings
    https://spacy.io/usage/embeddings-transformers
        - "Word vectors in spaCy are “static” in the sense that they are not learned parameters of the statistical models, and spaCy itself does not feature any algorithms for learning word vector tables. You can train a word vectors table using tools such as Gensim, FastText or GloVe, or download existing pretrained vectors. "

The reward goes to the previous step, that's why there is no reward in the first step
Also when selecting to stop (termination via action 4) it stops in the next step
Use the next_time_step.is_last().numpy() to end episode