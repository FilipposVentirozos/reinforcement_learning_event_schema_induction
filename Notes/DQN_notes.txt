OveralFor building RL environment
	- (Action) BoundedArraySpec, it's np.brodcast
		- take each minimum and maximum and makes it to a matrix of the given shape (if provided)
		- OR take the range of minimum and maximum and spread it 
		- I guess you can represent any 3D environment like that 
	- (Observation) ArraySpec
		- are the features length
	id = number of instances
	? episode_step = 0  # Episode step, resets every episode
	state = is the instance
	step
		y label is acted as reward to the action (BoundedArraySpec [1,0])
		then the reward is given based on the observation which are the features
		
		
RL_1 environment
	- Observation
		- Is the NE with some features (embedding) like in the IMDB
	- Action
		- Is [0,1]
	- Reward
	- Replay Buffer
		- https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c
			- We can prevent action values from oscillating or diverging catastrophically using a large 
				buffer of our past experience and sample training data from it, instead of using our latest experience.
			- The replay buffer contains a collection of experience tuples (S, A, R, S′)
			- The act of sampling a small batch of tuples from the replay buffer in order to learn is known as experience replay
		- We do this for every recipe.
	- id
		- is a random NE word
	- We give reward after each episode - a recipe
	- We give reward after each epoch - all the recipes are processed (LM type)
	- Balancing Multiple Sources of Reward in Reinforcement Learning (2000)
		- follow up
	- DCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING (2017)
		- seems similar that it has many QAs
	- Multi-agent: with sharing "logits"
	- Do I need batched size
	- I want rather TF environment rather than Python 
		- https://www.tensorflow.org/agents/tutorials/2_environments_tutorial#tensorflow_environments
	- I think this looks similar to what I want
		- Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery (2019)
			-https://github.com/011235813/hierarchical-marl
				- TF with no RL library
	- MARL papers
		- https://github.com/LantaoYu/MARL-Papers
	- Algorithm
		- Take a random seed which has not been seed before and prioritise non tagged NEs
		- For each random seed:
			- Take as action observation its neighbour based on normal distribution
				- The neighbour should not be a previous seed but it can be a previous tagged NE just not an observation that has been already on the same episode
		- Actions
			- To include or not (0,1)
		- Rewards
			- Penalise when adding a non NE
			- Penalise and reward episodes that are high or not all convoluted
			- Penalise by taking into account the number of main verbs of a recipe and that each episode should have the numer of NEs/main verbs
            - Use vanilla vs pre-train BERT to estimate the correct rewards (based on the paper: Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning )
                - For starters set all rewards equally divided by the total and 0 for the Os.
		- Rewards after many episodes, or epochs
			- For a certain group of recipes
				- Concatenate all the events, ordered-lessly
				- Build an MDP, LM for these events
				- If in the eval the MDP (LM) perfoms good then reward
				- We can also penalise if the MDP(LM) performs better in other domains instead of it's own
		- embeddings
			- contextualies with the sentence they belong to
				- or with Dep Tag
			- with sentence and recipe position
			- with the word itself - pretrain
		
Websites I am viewing:
https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb
https://github.com/tensorflow/agents/tree/master/docs/tutorials
https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb
	
General
https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e

https://ai.stackexchange.com/questions/11350/reinforcement-learning-with-long-term-rewards-and-fixed-states-and-actions
    Only one non-zero reward per thousand steps - this is referred to as "sparse rewards"
    I can return zero reward if chosen a simple NE
    Is called also: temporal credit assignment problem
        Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning (2019)
             (We presented a new algorithm for learning temporal credit assignment, which uses deep neural network (Transformer) to decompose the episodic reward back to each time step in the trajectory)
            - https://github.com/aadeshnpn/Temporal-Credit-Assignment
                - Downloaded on Zip

Discount factor gamma
    - https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning
        - I think I need a higher (closer to one) since I emphasize on future rewards
        - Or a zero since, the states are not inter-depended

Experience Replay
    https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits
        The learning phase is then logically separate from gaining experience, and based on taking random samples from this table.
        It is harder to use multi-step learning algorithms
            - Search that
    https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c
        - Replay buffer or Experience buffer
            - We can prevent action values from oscillating or diverging catastrophically using a large buffer of our past experience and sample training data from it, instead of using our latest experience.
        - The act of sampling a small batch of tuples from the replay buffer in order to learn is known as experience replay.
        - the basic idea behind experience replay is to storing past experiences and
            - then using a random subset of these experiences to update the Q-network,
                - rather than using just the single most recent experience
        - Is done using deque

        + Target Network
            - To make training more stable, there is a trick, called target network, by which we keep a copy of our neural network and use it for the Q(s’, a’) value in the Bellman equation.
                -  the target network’s parameters are not trained, but they are periodically synchronized with the parameters of the main Q-network
    https://github.com/tensorflow/agents/blob/master/docs/tutorials/5_replay_buffers_tutorial.ipynb
        - I think with the Replay Buffer I can run it after defined n steps for the RL to learn.
            - Which fits my case.

Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning (2019)
    - straightforward application of policy gradient methods, including REINFORCE [41], A2C [24] and PPO [32], may suffer from sample inefficiency, as the final episodic reward would only provide the same or similar supervision for learning policy over all time steps in a trajectory.
    - blackbox optimization approaches , such as cross entropy method [29], CMA-ES [11] and evolution strategies [30], have been also applied to episodic RL problems due to their computational efficiency.
    - Have a predictor that finds the best distribution of rewards in the action steps by minimising to the episodic reward (or the IntervalS)
    - What I want is an intermittent TCA since some are non NEs

    Learning Guidance Rewards with Trajectory-space Smoothing (2020)
        - Cite the above
        - In contrast with these, our computation of the guidance rewards does not require training auxiliary networks and could be viewed as a simple uniform return decomposition
        - A favorable property of our approach is that no additional neural networks need to be trained to obtain the guidance rewards, unlike recent works that also examine RL with delayed rewards
            - Will have https://github.com/tgangwani/GuidanceRewards

    Agent57: Outperforming the Atari Human Benchmark (2020)
        - Cite the above
        - Game of skiing seems relevant since it gets -5 after missing gates then receives in the end a final reward
        - Exploration algorithms in deep RL
            - NGU (Never Give Up)
        - Contributions
            - A new parameterization of the state-action value function that decomposes the contributions of the intrinsic and extrinsic rewards. As a result, we significantly increase the training stability over a large range of intrinsic reward scales.
            - This allows the agent to control the exploration/exploitation trade-off by dedicating more resources to one or the other.

Policies
    - https://stackoverflow.com/a/46269757/5455013
        - Which action should the RL take?
        - I think in my case, start with random

https://github.com/tensorflow/agents/blob/master/docs/tutorials/4_drivers_tutorial.ipynb
    - Drivers execute a policy over a specific number of steps or episodes. Should I use that in my task?
        - Does the RL learn after the driver has been executed? Is the reward given after that?

States could depend on the action taken, in my case it does not depend

Offline RL (https://slideslive.com/38935785/offline-reinforcement-learning-from-algorithms-to-practical-challenges)
    - good for when to prioritise distributions over good and bad behaviours
        - For tweaking the policy
    - Good for when there is alot of data on a subject like self driving cars
        - Make use of offline data, instead of online
        - There could be a lot of data show the correct trajectory but the agent cannot test other trajectories
            - leading to compounding errors
        - Distribution shift is an issue, see minute 40 for a good example
    - I think offline RL is not useful here, follow up later
    - paper resource: https://arxiv.org/pdf/2005.01643.pdf (Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems)
    - https://github.com/tensorflow/agents/issues/405
    - I think offline RL is similar because the agent cannot choose the new state. But is different because it can choose the action which changes the reward.

- Read chpater 5 https://ebookcentral.proquest.com/lib/manchester/reader.action?docID=5439844&ppg=104
    - TD Learning
        - approximates current estimate based on previous learned estimates, unlike Monte Carlo
            - (a.k.a. boostraping)
        - MC takes the mean return
        - It is about updating the reward based on the next state
            - Looks for actions that has the highest reward then it takes that action leading to a new state
        - I think it does not apply to our case, since whichever action we take it will always land to the same new state.
        - Sarsa vs Q learning I think differs on exploring options rather than taking the max
            - Exactly, the difference between on and off policy. Sarsa takes the future state reward whereas Q learning sets what was the max there.

Chapter 6 https://ebookcentral.proquest.com/lib/manchester/reader.action?docID=5439844&ppg=104
    - MAB
        - Different exploration techniques
Chapter 8
    - I think is about the expansions of chapter 5 Q network with more Deep version where the states are convoluted.
Chapter 9
    - partially observable Markov Decision Process ( POMDP )
        - is when we only know part of the world
    - Uses LSTM to model a state with the previous states
    - With Attention as well
    - I could use something similar between each state of words?

Chapter 10
    - A3C, many agents which update the global network
        - Cause of that multi-agent structure no use of replay memory

Chapter 12 Racing
    - I think I can use the target network to update the rewards in my case?




Extracting Action Sequences from Texts Based on Deep Reinforcement Learning (2018)
    - Q-networks to build the Q-functions.
    - trains two networks one for verbs and one for their args, where the latter uses the input of the first
    - CNN
        - design four types of kernels, which correspond to bigram, trigram, four-gram and five-gram, respectively.]
    https://github.com/Fence/EASDRL/blob/master/Environment.py
        - elif set rewards
        - Move to new state
            - self.state[word_ind, -1] = action + 1
        - self.state = self.text_vec.copy()
            - I assume the step just goes to the next word

I think my case looks like the MAB
    - https://stats.stackexchange.com/questions/379644/actions-with-no-effect-on-the-enviroment-in-rl

RL_algos procedure:
    - Policy
        - which binary action to take?
    - Exploration
        - Sequential, Normal distribution or other?
    -

Sequence labeling with Reinforcement Learning and Ranking Algorithms (2007)
    - Order the ones first which our agent is the most certain

Continuous space
    - https://ai.stackexchange.com/questions/12255/can-q-learning-be-used-for-continuous-state-or-action-spaces
    - Do I need to encode the embeddings to a sub states or use an Deep learning mapping function

Ensemble NER with Q Network
    - https://link-springer-com.manchester.idm.oclc.org/chapter/10.1007/978-3-030-29894-4_13
    - Document-Level Named Entity Recognition with Q-Network
    - His code is weird

Joint Entity and Relation Extraction with a Hybrid Transformer and Reinforcement Learning Based Model (2020)
    - Considers one triplet possible per sentence
    - I think it uses a approximator function for the state, which takes into account different sentence features
    - I think does not have code

DeNERT-KG: Named Entity and Relation Extraction Model Using DQN, Knowledge Graph, and BERT (2020)
    - An epoch is when all sequences are labelled in a sentece


Deep Reinforcement Learning with a Natural Language Action Space (2016)
    - DRRN function approximator
        - to train
            - Uses experience replay
                -  compute the temporal difference error for sample k
            - Can my sentence embeddings algo help to converge the space
        - It's about text based RPG games

Distantly Supervised NER with Partial Annotation Learning and Reinforcement Learning (2018)
    - Selecting clean instances for the NER, not very clear, maybe revisit later
        - https://github.com/rainarch/DSNER

NATURAL LANGUAGE STATE REPRESENTATION FOR REINFORCEMENT LEARNING (2019 ICLR)
    - good to cite, it seems that language is better for RL cause is more concise
        - was rejected

Learn How to Cook a New Recipe in a New House: Using Map Familiarization, Curriculum Learning, and Bandit Feedback to Learn Families of Text-Based Adventure Games (2020)
    - it's a game setting
        - maybe revisit later

Delayed rewards
    - https://github.com/tensorflow/agents/issues/529

In Conversational AI
    - The state is constructed from the information the last User action as well as the last Agent action.

RL tutorial
    https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#FunctionApproximation


Curriculum Learning
    - Is an interesting section when learning the cooking actions
    - Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey
        - Is about having a teacher and learning the best sequence of actions


https://medium.com/swlh/states-observation-and-action-spaces-in-reinforcement-learning-569a30a8d2a1
    -  For example the DDPG algorithm can only be applied to environments with continuous action space, while the PPO algorithm can be used for environments with either discrete or continuous action spaces.
    - I try an agent for continuous envs

https://github.com/ConvLab/ConvLab/

Some DRL for Dialogues just add the word embeddings together for the states, plus concatenate the previous state
    - https://github.com/KGSands/DRL-Chatbot/blob/master/chatbot.py

PPO agent contructor
    - https://github.com/rmsander/marl_ppo/blob/main/ppo/utils.py