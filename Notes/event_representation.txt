event representation
    - have a distance between each NE (verb, ingr) of the predicted and target event
        - Count the distances
    -  Can use simple Glove Embeddings on the hypernyms
        - take lemma of each NE
    - How to combine multiple NEs of the same type
        - Use some custom heuristics for starters
            - the common hypernym, or average of tw
    - What it we use some sort of clustering technique to cut down the amount of unique words
        - Probably a distance based clustering technique
    - Need to have a function to calculate the event average position
    - Use VAE to generate also for each type, nested type
        - Or could be to bring the threshold to a level where if it's above that then include those
        - Could apply also some common sense rules when generating NEs
    - Encoding and then feeding to whatever sequence learner
        - Can easily encode different types
        - Have the risk of running into too many different types making it not enough data to produce
            - A workaround could be to cluster the NEs
                - Cluster each NE type
                    - To cluster ingrs
                        - First find the average of each embedding ingr group (based on labels)
                        - Then cluster
            - For multiple NEs
                - Do one pass after the training and identify the common events which have at least 2, 3 (apriori algorithm)
                - Encode the events with the most possible matches
            - N-grams of NEs
                - take the average I guess




## Process 1
Teach an RL to measure how events differ based on the Semantic role labels used and the Named entities, the graph as well.
Steps:
1) Have to mine the data examples to find the event structures, the labelled instances
2) Distances between events can be set in the manner on how the recipes differ and to what step of the process they occur.
3) Learn how to weigh the distances between events
    a) That could be done by using RL
4) Learn event sequence probability

## Process 3
1) Get events from Process 1 and parse to DX-MAN and to BTs

## Process 2
Take the event and represent it as graph and take into account the word embeddings used.
Then find
Steps:
1) RL to find which words match together to form an event based on a series of rewards
  a) Local rewards, such as NER, verb number, convolution between neighbouring RL episodes
  b) Have a predictor to advise the reward on how probable is to form an event from Process 1. That would
     be based on what's the probability to match a current word with the words of the same episode in the
     context of neighbouring extracted events.
2) Learn event sequences, after the RL has extracted the new events
3) Evaluate on similar recipes
    a) Do 2 & 3 iteratively to evaluate the RL for adding new events
3) Update and evaluate the sequence model
